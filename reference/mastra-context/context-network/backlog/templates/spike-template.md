# Spike: {{TITLE}}

**ID**: spike-{{SPIKE_ID}}
**Type**: Research / Investigation / Proof of Concept
**Priority**: {{PRIORITY}}
**Status**: {{STATUS}}
**Assignee**: {{ASSIGNEE}}
**Time Box**: {{TIME_BOX}}
**Created**: {{CREATED_DATE}}
**Updated**: {{UPDATED_DATE}}

## Objective

Clear statement of what this spike aims to discover, investigate, or prove.

## Background

### Context
Why is this spike necessary?
- What uncertainty or unknowns exist?
- What decisions depend on this research?
- How does this relate to current work?

### Business Justification
- **Impact**: What business decisions depend on this?
- **Risk**: What risks does this research mitigate?
- **Opportunity**: What opportunities might this unlock?

### Technical Context
- **Current State**: What do we know already?
- **Gaps**: What specific knowledge gaps exist?
- **Constraints**: What technical limitations apply?

## Research Questions

List specific questions this spike will answer:

### Primary Questions
1. **Question 1**: [Specific, answerable question]
2. **Question 2**: [Specific, answerable question]
3. **Question 3**: [Specific, answerable question]

### Secondary Questions
1. **Question 1**: [Lower priority but valuable question]
2. **Question 2**: [Lower priority but valuable question]

### Success Criteria
How will we know this spike was successful?
- [ ] Primary questions answered with confidence
- [ ] Clear recommendation can be made
- [ ] Sufficient evidence gathered for decision-making
- [ ] Risks and trade-offs are well understood

## Research Plan

### Investigation Approach
Describe the methodology for the research:

#### Literature Review
- [ ] Existing documentation review
- [ ] Industry best practices research
- [ ] Competitive analysis
- [ ] Academic research review

#### Technical Investigation
- [ ] Code analysis and review
- [ ] Architecture evaluation
- [ ] Performance testing
- [ ] Security assessment
- [ ] Prototype development

#### Stakeholder Input
- [ ] Expert interviews
- [ ] Team consultations
- [ ] User feedback collection
- [ ] Customer interviews

### Research Activities

#### Week 1: [Activity Group]
- **Day 1-2**: [Specific activities]
- **Day 3-4**: [Specific activities]
- **Day 5**: [Specific activities]

#### Week 2: [Activity Group]
- **Day 1-2**: [Specific activities]
- **Day 3-4**: [Specific activities]
- **Day 5**: [Specific activities]

### Deliverables
What will be produced by this spike:
- [ ] Research summary document
- [ ] Technical findings report
- [ ] Prototype or proof of concept
- [ ] Recommendation with rationale
- [ ] Risk assessment
- [ ] Next steps proposal

## Hypotheses

### Primary Hypothesis
**Hypothesis**: [What you expect to find]
**Rationale**: [Why you expect this]
**Test Method**: [How you'll validate/invalidate this]

### Alternative Hypotheses
1. **Hypothesis**: [Alternative possibility]
   **Rationale**: [Why this might be true]
   **Test Method**: [How to test this]

2. **Hypothesis**: [Another alternative]
   **Rationale**: [Why this might be true]
   **Test Method**: [How to test this]

## Scope and Constraints

### In Scope
What this spike will investigate:
- [Specific area 1]
- [Specific area 2]
- [Specific area 3]

### Out of Scope
What this spike will NOT investigate:
- [Specific exclusion 1]
- [Specific exclusion 2]
- [Specific exclusion 3]

### Time Constraints
- **Time Box**: [Maximum time to spend]
- **Deadline**: [When results are needed]
- **Checkpoints**: [Regular review points]

### Resource Constraints
- **Budget**: [Any budget limitations]
- **Tools**: [Tools and resources available]
- **Access**: [System or data access needed]
- **Expertise**: [External expertise required]

## Mastra-Specific Research Areas

If this spike relates to Mastra components, specify areas of investigation:

### Agent Research
- [ ] Agent architecture patterns
- [ ] Tool composition strategies
- [ ] Memory management approaches
- [ ] Performance characteristics
- [ ] Scalability considerations

### Workflow Research
- [ ] Workflow engine capabilities
- [ ] Step execution patterns
- [ ] Suspend/resume mechanisms
- [ ] Error handling strategies
- [ ] Performance optimization

### Tool Research
- [ ] Tool development patterns
- [ ] Schema validation approaches
- [ ] Error handling strategies
- [ ] Performance considerations
- [ ] Integration patterns

### Integration Research
- [ ] MCP server capabilities
- [ ] A2A communication patterns
- [ ] API design approaches
- [ ] Authentication strategies
- [ ] Security considerations

### Storage Research
- [ ] Storage provider options
- [ ] Memory system architecture
- [ ] Vector storage solutions
- [ ] Caching strategies
- [ ] Data migration approaches

## Risk Assessment

### Research Risks
Risks related to the spike itself:

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| [Risk description] | High/Med/Low | High/Med/Low | [Mitigation strategy] |
| [Risk description] | High/Med/Low | High/Med/Low | [Mitigation strategy] |

### Decision Risks
Risks of making decisions without this research:

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| [Risk description] | High/Med/Low | High/Med/Low | [Mitigation strategy] |
| [Risk description] | High/Med/Low | High/Med/Low | [Mitigation strategy] |

### Implementation Risks
Potential risks identified through research:

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| [Risk description] | High/Med/Low | High/Med/Low | [Mitigation strategy] |
| [Risk description] | High/Med/Low | High/Med/Low | [Mitigation strategy] |

## Research Methods

### Data Collection
How will you gather information?

#### Quantitative Methods
- **Performance Testing**: [specific tests to run]
- **Benchmarking**: [benchmarks to conduct]
- **Metrics Analysis**: [metrics to collect]
- **Load Testing**: [load scenarios to test]

#### Qualitative Methods
- **Interviews**: [who to interview and questions]
- **Code Review**: [code areas to analyze]
- **Documentation Review**: [docs to examine]
- **Expert Consultation**: [experts to consult]

#### Experimental Methods
- **Prototyping**: [prototypes to build]
- **A/B Testing**: [tests to conduct]
- **Proof of Concept**: [concepts to prove]
- **Pilot Implementation**: [pilot areas to test]

### Validation Criteria
How will you validate findings?
- **Evidence Quality**: [standards for evidence]
- **Source Credibility**: [how to assess sources]
- **Reproducibility**: [how to verify results]
- **Peer Review**: [who will review findings]

## Expected Outcomes

### Possible Scenarios

#### Best Case Scenario
- **Finding**: [Most favorable outcome]
- **Implication**: [What this means for the project]
- **Next Steps**: [What would happen next]

#### Most Likely Scenario
- **Finding**: [Most probable outcome]
- **Implication**: [What this means for the project]
- **Next Steps**: [What would happen next]

#### Worst Case Scenario
- **Finding**: [Least favorable outcome]
- **Implication**: [What this means for the project]
- **Next Steps**: [What would happen next]

### Decision Points
Key decisions this spike will inform:
1. **Decision 1**: [Decision to be made]
   - **Options**: [Available choices]
   - **Criteria**: [Decision criteria]

2. **Decision 2**: [Decision to be made]
   - **Options**: [Available choices]
   - **Criteria**: [Decision criteria]

## Knowledge Management

### Documentation Plan
How will findings be documented and shared?

#### Research Notes
- **Daily Notes**: [where daily progress is recorded]
- **Meeting Notes**: [stakeholder meetings and interviews]
- **Experiment Logs**: [technical experiments and results]

#### Final Deliverables
- **Executive Summary**: [high-level findings for leadership]
- **Technical Report**: [detailed technical findings]
- **Recommendations**: [actionable next steps]
- **Supporting Evidence**: [data, code, references]

#### Knowledge Sharing
- **Team Presentation**: [present findings to team]
- **Documentation Updates**: [update project documentation]
- **Best Practices**: [capture best practices learned]
- **Lessons Learned**: [document lessons for future spikes]

### Information Sources
Track all sources used in research:

#### Internal Sources
- [ ] Existing codebase analysis
- [ ] Internal documentation
- [ ] Team member expertise
- [ ] Previous project experience

#### External Sources
- [ ] Industry research and reports
- [ ] Academic papers and studies
- [ ] Open source projects
- [ ] Vendor documentation
- [ ] Community discussions

## Progress Tracking

### Daily Progress Log
Track daily research activities:

#### {{DATE}}
- **Activities**: [what was done]
- **Findings**: [what was discovered]
- **Questions**: [new questions raised]
- **Next Steps**: [planned activities]

#### {{DATE}}
- **Activities**: [what was done]
- **Findings**: [what was discovered]
- **Questions**: [new questions raised]
- **Next Steps**: [planned activities]

### Weekly Checkpoints
Regular progress assessment:

#### Week 1 Checkpoint
- **Progress**: [% complete, goals met]
- **Key Findings**: [important discoveries]
- **Blockers**: [obstacles encountered]
- **Adjustments**: [plan modifications needed]

#### Week 2 Checkpoint
- **Progress**: [% complete, goals met]
- **Key Findings**: [important discoveries]
- **Blockers**: [obstacles encountered]
- **Adjustments**: [plan modifications needed]

### Milestone Tracking
- [ ] Research plan approved
- [ ] Initial investigation complete
- [ ] Key experiments conducted
- [ ] Stakeholder interviews completed
- [ ] Prototype/POC developed
- [ ] Findings documented
- [ ] Recommendations formulated
- [ ] Results presented

## Research Results

### Key Findings
Document major discoveries:

#### Finding 1: [Title]
- **Evidence**: [supporting data/evidence]
- **Confidence**: High / Medium / Low
- **Implications**: [what this means]
- **Recommendations**: [suggested actions]

#### Finding 2: [Title]
- **Evidence**: [supporting data/evidence]
- **Confidence**: High / Medium / Low
- **Implications**: [what this means]
- **Recommendations**: [suggested actions]

#### Finding 3: [Title]
- **Evidence**: [supporting data/evidence]
- **Confidence**: High / Medium / Low
- **Implications**: [what this means]
- **Recommendations**: [suggested actions]

### Answers to Research Questions
Map findings back to original questions:

#### Primary Questions
1. **Question**: [original question]
   **Answer**: [detailed answer]
   **Confidence**: [confidence level]

2. **Question**: [original question]
   **Answer**: [detailed answer]
   **Confidence**: [confidence level]

#### Secondary Questions
1. **Question**: [original question]
   **Answer**: [detailed answer]
   **Confidence**: [confidence level]

### Unexpected Discoveries
Document surprises and unplanned findings:
- **Discovery 1**: [unexpected finding]
- **Discovery 2**: [unexpected finding]
- **Discovery 3**: [unexpected finding]

## Recommendations

### Primary Recommendation
**Recommendation**: [main recommended action]
**Rationale**: [why this is recommended]
**Evidence**: [supporting evidence]
**Risks**: [associated risks]
**Implementation**: [how to implement]

### Alternative Recommendations
1. **Recommendation**: [alternative option]
   **Rationale**: [reasoning]
   **Trade-offs**: [pros and cons]

2. **Recommendation**: [another alternative]
   **Rationale**: [reasoning]
   **Trade-offs**: [pros and cons]

### Implementation Plan
If proceeding with recommendations:
1. **Phase 1**: [immediate actions]
2. **Phase 2**: [medium-term actions]
3. **Phase 3**: [long-term actions]

### Follow-up Research
Additional spikes or research needed:
- **Spike 1**: [follow-up research area]
- **Spike 2**: [follow-up research area]
- **Spike 3**: [follow-up research area]

## Lessons Learned

### Research Process
What worked well and what didn't:
- **Effective Methods**: [successful research approaches]
- **Ineffective Methods**: [approaches that didn't work]
- **Process Improvements**: [how to improve next time]

### Knowledge Gaps
Remaining unknowns:
- **Gap 1**: [area still unclear]
- **Gap 2**: [area still unclear]
- **Gap 3**: [area still unclear]

### Future Considerations
Important factors for future work:
- **Consideration 1**: [factor to keep in mind]
- **Consideration 2**: [factor to keep in mind]
- **Consideration 3**: [factor to keep in mind]

---

## Template Usage Instructions

When creating a new spike:

1. **Copy this template** to `context-network/backlog/active/spike-[ID]-[name].md`
2. **Define clear objectives** and research questions
3. **Set realistic time boundaries** - spikes should be time-boxed
4. **Focus on answering specific questions** rather than building solutions
5. **Document everything** - the learning is as valuable as the conclusions
6. **Share results widely** - spikes benefit the entire team

### Spike Types

**Research Spike**:
- Investigating new technologies or approaches
- Understanding domain complexity
- Evaluating third-party solutions

**Technical Spike**:
- Proving technical feasibility
- Performance testing and benchmarking
- Architecture validation

**Design Spike**:
- User experience research
- Interface design exploration
- Usability testing

### Time Boxing Guidelines

**Small Spike (1-3 days)**:
- Simple technical questions
- Quick feasibility checks
- Tool evaluation

**Medium Spike (1 week)**:
- Architecture evaluation
- Performance analysis
- Integration research

**Large Spike (2 weeks)**:
- Complex technical research
- Comprehensive solution evaluation
- Multi-component analysis

### Success Criteria

A successful spike should:
- Answer the original research questions
- Provide actionable recommendations
- Reduce uncertainty for future work
- Generate reusable knowledge
- Stay within time and scope boundaries